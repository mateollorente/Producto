{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateollorente/Producto/blob/master/beataiVNum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "github_user = userdata.get(\"github_user\")\n",
        "github_token = userdata.get(\"GITHUB_TOKEN\")\n",
        "github_mail = userdata.get(\"github_mail\")\n",
        "!git config --global user.name \"{github_user}\"\n",
        "!git config --global user.email \"{github_mail}\"\n",
        "repo_url = f\"https://{github_user}:{github_token}@github.com/SantiagoBuffa/BeatAI.git\"\n",
        "!git remote set-url origin $repo_url\n",
        "\n",
        "!git clone https://github.com/SantiagoBuffa/BeatAI.git\n",
        "%cd BeatAI"
      ],
      "metadata": {
        "id": "EGvxjaKHzUxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8083c5-7364-4fef-effb-f55df14c0295"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: Unable to read current working directory: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: Unable to read current working directory: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: Unable to read current working directory: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: could not create work tree dir 'BeatAI': No such file or directory\n",
            "[Errno 2] No such file or directory: 'BeatAI'\n",
            "/content/BeatAI/BeatAI/BeatAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os, json\n",
        "\"\"\"\n",
        "kaggle_username = userdata.get(\"kaggle_username\")\n",
        "kaggle_key = userdata.get(\"kaggle_key\")\n",
        "\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n",
        "    json.dump({\"username\": kaggle_username, \"key\": kaggle_key}, f)\n",
        "\n",
        "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
        "\n",
        "!pip install kaggle --quiet\n",
        "!kaggle datasets download -d evilspirit05/ecg-analysis -p ./data --unzip\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "a4XsCfJf5gEg",
        "outputId": "69d994b9-04f6-497f-d42a-9162ec35c811"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nkaggle_username = userdata.get(\"kaggle_username\")\\nkaggle_key = userdata.get(\"kaggle_key\")\\n\\nos.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\\nwith open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\\n    json.dump({\"username\": kaggle_username, \"key\": kaggle_key}, f)\\n\\nos.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\\n\\n!pip install kaggle --quiet\\n!kaggle datasets download -d evilspirit05/ecg-analysis -p ./data --unzip\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# --- DEFINICIONES GLOBALES ---\n",
        "# Se recomienda mantener estas constantes fuera de la función\n",
        "SEQUENCE_LENGTH = 1000 # Longitud deseada de la serie de tiempo 1D\n",
        "THRESHOLD_VALUE = 50   # Umbral de binarización (ajustar si es necesario)\n",
        "# -----------------------------\n",
        "\n",
        "def preprocess_ecg_image_final(img, sequence_length=SEQUENCE_LENGTH):\n",
        "\n",
        "    # --- 1. Normalización de forma a Gris (H, W) ---\n",
        "    if img.ndim == 3:\n",
        "        if img.shape[-1] == 3:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        elif img.shape[-1] == 4:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGBA2GRAY)\n",
        "        elif img.shape[-1] == 1:\n",
        "            img = np.squeeze(img, axis=-1)\n",
        "    elif img.ndim != 2:\n",
        "        raise ValueError(f\"Formato de imagen inesperado: {img.shape}\")\n",
        "\n",
        "    original_height, original_width = img.shape\n",
        "\n",
        "    # --- 2. Umbralización para AISLAR la señal ---\n",
        "    # Convertimos a 8-bit para cv2\n",
        "    img_8bit = img.astype(np.uint8) if img.dtype != np.uint8 else img\n",
        "\n",
        "    # THRESH_BINARY_INV: Lo oscuro (la onda) se vuelve BLANCO (255), el resto NEGRO (0).\n",
        "    _, img_binarized = cv2.threshold(\n",
        "        img_8bit,\n",
        "        THRESHOLD_VALUE,\n",
        "        255,\n",
        "        cv2.THRESH_BINARY_INV\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # --- 3. Segmentación: Recorte de Región de Interés (ROI) Fijo ---\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    # Estos porcentajes definen el área central donde están las ondas\n",
        "    y_start = int(original_height * 0.15)\n",
        "    y_end = int(original_height * 0.90)\n",
        "    x_start = int(original_width * 0.05)\n",
        "    x_end = int(original_width * 0.95)\n",
        "\n",
        "    # 💥 INICIALIZACIÓN CLAVE: img_cropped se define AQUI\n",
        "    img_cropped = img_binarized[y_start:y_end, x_start:x_end]\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # --- 4. Recorte automático de bordes basado en la señal ---\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    coords_final = cv2.findNonZero(img_cropped)\n",
        "\n",
        "    # Si encuentra coordenadas, actualiza img_cropped.\n",
        "    if coords_final is not None:\n",
        "        x, y, w, h = cv2.boundingRect(coords_final)\n",
        "        margin = 2\n",
        "        x = max(0, x - margin)\n",
        "        y = max(0, y - margin)\n",
        "        w = min(img_cropped.shape[1] - x, w + 2 * margin)\n",
        "        h = min(img_cropped.shape[0] - y, h + 2 * margin)\n",
        "\n",
        "        # Sobreescribe img_cropped con la versión final recortada\n",
        "        img_cropped = img_cropped[y:y+h, x:x+w]\n",
        "    # Si coords_final is None, img_cropped mantiene el valor asignado en el Paso 3.\n",
        "    # ¡La variable está garantizada!\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # --- 5. Redimensionar SOLO el Ancho a SEQUENCE_LENGTH ---\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    final_width = sequence_length\n",
        "\n",
        "    # Esta línea ahora es segura porque img_cropped existe\n",
        "    if img_cropped.shape[0] == 0 or img_cropped.shape[1] == 0:\n",
        "        img_resized = np.zeros((1, final_width), dtype=np.uint8)\n",
        "    else:\n",
        "      new_height = img_cropped.shape[0]\n",
        "\n",
        "      # Redimensionar: Se mantiene la altura original (new_height) y solo se fuerza el ancho (final_width).\n",
        "      img_resized = cv2.resize(\n",
        "          img_cropped,\n",
        "          (final_width, new_height),\n",
        "          interpolation=cv2.INTER_LINEAR\n",
        "      )\n",
        "\n",
        "    # Normalizamos a float [0.0, 1.0]\n",
        "    img_final = img_resized.astype(np.float32) / 255.0\n",
        "\n",
        "    return img_final"
      ],
      "metadata": {
        "id": "x-PhtNTzYQ3l"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "hpxvGDPxzgUR"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Asegúrate de que estas variables estén definidas fuera de la función\n",
        "SEQUENCE_LENGTH = 1000\n",
        "THRESHOLD_VALUE = 50\n",
        "\n",
        "def preprocess_ecg_image_final(img, sequence_length=SEQUENCE_LENGTH):\n",
        "\n",
        "    # --- 1. Normalización de forma a Gris (H, W) ---\n",
        "    if img.ndim == 3:\n",
        "        if img.shape[-1] == 3:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        elif img.shape[-1] == 4:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGBA2GRAY)\n",
        "        elif img.shape[-1] == 1:\n",
        "            img = np.squeeze(img, axis=-1)\n",
        "    elif img.ndim != 2:\n",
        "        raise ValueError(f\"Formato de imagen inesperado: {img.shape}\")\n",
        "\n",
        "    original_height, original_width = img.shape\n",
        "\n",
        "    # --- 2. Umbralización para AISLAR la señal ---\n",
        "    img_8bit = img.astype(np.uint8) if img.dtype != np.uint8 else img\n",
        "\n",
        "    _, img_binarized = cv2.threshold(\n",
        "        img_8bit,\n",
        "        THRESHOLD_VALUE,\n",
        "        255,\n",
        "        cv2.THRESH_BINARY_INV\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # --- 3. Segmentación: Recorte de Región de Interés (ROI) Fijo ---\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    y_start = int(original_height * 0.15)\n",
        "    y_end = int(original_height * 0.90)\n",
        "    x_start = int(original_width * 0.05)\n",
        "    x_end = int(original_width * 0.95)\n",
        "\n",
        "    # 💥 INICIALIZACIÓN/DEFINICIÓN BASE: img_cropped siempre se define aquí\n",
        "    img_cropped = img_binarized[y_start:y_end, x_start:x_end]\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # --- 4. Recorte automático de bordes basado en la señal ---\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    coords_final = cv2.findNonZero(img_cropped)\n",
        "\n",
        "    # Si se encuentran coordenadas, re-recortamos la imagen\n",
        "    if coords_final is not None:\n",
        "        x, y, w, h = cv2.boundingRect(coords_final)\n",
        "        margin = 2\n",
        "        x = max(0, x - margin)\n",
        "        y = max(0, y - margin)\n",
        "        w = min(img_cropped.shape[1] - x, w + 2 * margin)\n",
        "        h = min(img_cropped.shape[0] - y, h + 2 * margin)\n",
        "\n",
        "        # Sobreescribe img_cropped con la versión final recortada\n",
        "        img_cropped = img_cropped[y:y+h, x:x+w]\n",
        "    # Si coords_final es None, img_cropped mantiene el valor del Paso 3. ¡Solucionado!\n",
        "\n",
        "    # -------------------------------------------------------------\n",
        "    # --- 5. Redimensionar SOLO el Ancho a SEQUENCE_LENGTH ---\n",
        "    # -------------------------------------------------------------\n",
        "\n",
        "    final_width = sequence_length\n",
        "\n",
        "    # Esta línea ahora es segura porque img_cropped existe\n",
        "    if img_cropped.shape[0] == 0 or img_cropped.shape[1] == 0:\n",
        "        img_resized = np.zeros((1, final_width), dtype=np.uint8)\n",
        "    else:\n",
        "      new_height = img_cropped.shape[0]\n",
        "\n",
        "      img_resized = cv2.resize(\n",
        "          img_cropped,\n",
        "          (final_width, new_height),\n",
        "          interpolation=cv2.INTER_LINEAR\n",
        "      )\n",
        "\n",
        "    # Normalizamos a float [0.0, 1.0]\n",
        "    img_final = img_resized.astype(np.float32) / 255.0\n",
        "\n",
        "    return img_final"
      ],
      "metadata": {
        "id": "9u386vcxdNNw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCIÓN 2: EXTRACCIÓN DE SEÑAL 1D ---\n",
        "def extract_signal_from_binarized_ecg(img_signal):\n",
        "    \"\"\"\n",
        "    Escanea la imagen binarizada columna por columna para extraer la serie de tiempo.\n",
        "    Asume: img_signal es (H, W), con Señal=1.0 y Fondo=0.0.\n",
        "    \"\"\"\n",
        "    height, width = img_signal.shape\n",
        "    signal_series = np.zeros(width)\n",
        "\n",
        "    # Recorrer cada columna (eje X = tiempo)\n",
        "    for col in range(width):\n",
        "        column_data = img_signal[:, col]\n",
        "\n",
        "        # Encontrar los índices Y (posiciones verticales) donde hay señal (valor == 1.0)\n",
        "        signal_indices = np.where(column_data > 0.5)[0]\n",
        "\n",
        "        if len(signal_indices) > 0:\n",
        "            # Usamos el centroide de la línea\n",
        "            y_position = np.mean(signal_indices)\n",
        "\n",
        "            # Invertimos la posición Y para que los picos altos de la onda\n",
        "            # (que tienen un valor Y bajo en la imagen) tengan un valor alto en la serie.\n",
        "            signal_series[col] = height - y_position\n",
        "        else:\n",
        "            # Si no hay píxeles de señal, usar un valor interpolado (ej. la última posición)\n",
        "            signal_series[col] = signal_series[col-1] if col > 0 else (height / 2)\n",
        "\n",
        "    # Normalizar la serie de tiempo final (Min-Max a [0, 1])\n",
        "    if np.max(signal_series) > np.min(signal_series):\n",
        "        signal_series = (signal_series - np.min(signal_series)) / (np.max(signal_series) - np.min(signal_series))\n",
        "\n",
        "    # Keras espera (pasos_de_tiempo, caracteristicas), aquí (W, 1)\n",
        "    return np.expand_dims(signal_series, axis=-1)"
      ],
      "metadata": {
        "id": "BnAisJ5YdUwb"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 1000\n",
        "INITIAL_TARGET_SIZE = (1000, 800)\n",
        "# Función de preprocesamiento total para el ImageDataGenerator\n",
        "def combined_preprocessing(img):\n",
        "    \"\"\"\n",
        "    Aplica la limpieza 2D y luego la conversión a serie de tiempo 1D.\n",
        "    Esta es la función que pasas al ImageDataGenerator.\n",
        "    \"\"\"\n",
        "    # 1. Limpieza y recorte 2D, redimensiona el ancho a SEQUENCE_LENGTH\n",
        "    img_clean_2d = preprocess_ecg_image_final(img, sequence_length=SEQUENCE_LENGTH)\n",
        "\n",
        "    # 2. Extrae la señal 1D del resultado\n",
        "    signal_1d = extract_signal_from_binarized_ecg(img_clean_2d)\n",
        "\n",
        "    return signal_1d"
      ],
      "metadata": {
        "id": "A_f7b-U9dZgJ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 1000\n",
        "INITIAL_TARGET_SIZE = (1000, 800)\n",
        "def preprocess_dataset(dataset_path):\n",
        "    train_dir = os.path.join(dataset_path, 'train')\n",
        "    test_dir = os.path.join(dataset_path, 'test')\n",
        "\n",
        "    # 1. GENERADOR DE ENTRENAMIENTO Y VALIDACIÓN\n",
        "    # Usamos validation_split para decirle al generador que separe el 20% de los datos\n",
        "    train_val_datagen = ImageDataGenerator(\n",
        "        preprocessing_function=combined_preprocessing,\n",
        "        validation_split=0.20 # <--- ¡CLAVE! 20% para validación\n",
        "    )\n",
        "\n",
        "    # GENERADOR DE ENTRENAMIENTO (usa subset='training')\n",
        "    train_generator = train_val_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=INITIAL_TARGET_SIZE,\n",
        "        color_mode='grayscale',\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='training' # <--- Datos de entrenamiento\n",
        "    )\n",
        "\n",
        "    # GENERADOR DE VALIDACIÓN (usa subset='validation')\n",
        "    validation_generator = train_val_datagen.flow_from_directory(\n",
        "        train_dir, # Usa la MISMA carpeta 'train'\n",
        "        target_size=INITIAL_TARGET_SIZE,\n",
        "        color_mode='grayscale',\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='validation' # <--- Datos de validación\n",
        "    )\n",
        "\n",
        "    # 2. GENERADOR DE PRUEBA (TEST)\n",
        "    # No necesita el validation_split. Solo necesita el preprocessing.\n",
        "    test_datagen = ImageDataGenerator(\n",
        "        preprocessing_function=combined_preprocessing\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=INITIAL_TARGET_SIZE,\n",
        "        color_mode='grayscale',\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    return train_generator, validation_generator, test_generator"
      ],
      "metadata": {
        "id": "gcsAzm0Mdh0T"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 1000 # Debe coincidir con el valor usado arriba\n",
        "NUM_CLASSES = 4        # Ajusta esto al número de clases de tu problema\n",
        "\n",
        "def build_ecg_cnn1d(sequence_length=SEQUENCE_LENGTH, num_classes=NUM_CLASSES):\n",
        "    model = Sequential([\n",
        "        # input_shape = (longitud_secuencia, caracteristicas_por_paso)\n",
        "        Conv1D(32, kernel_size=16, activation='relu', input_shape=(sequence_length, 1)),\n",
        "        MaxPooling1D(pool_size=4),\n",
        "        Conv1D(64, kernel_size=16, activation='relu'),\n",
        "        MaxPooling1D(pool_size=4),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-4),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Ejemplo de Uso ---\n",
        "# model_1d = build_ecg_cnn1d()\n",
        "# model_1d.summary()\n",
        "# history = model_1d.fit(train_generator, epochs=10, validation_data=test_generator)"
      ],
      "metadata": {
        "id": "WXns7MhxGzvm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ✅ Dataset path\n",
        "data_dir = 'data/ECG_DATA'\n",
        "train_generator, val_generator, test_generator = preprocess_dataset(data_dir)\n",
        "\n",
        "# ✅ Modelo\n",
        "model = build_ecg_cnn1d()\n",
        "model.summary()\n",
        "history = model.fit(train_generator, epochs=10, validation_data=val_generator)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f'Test accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "8gSRW01TBn-h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "966959e7-b675-4c4b-9297-55ea24b61f33"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/ECG_DATA/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-525913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ✅ Dataset path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/ECG_DATA'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ✅ Modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1016681863.py\u001b[0m in \u001b[0;36mpreprocess_dataset\u001b[0;34m(dataset_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# GENERADOR DE ENTRENAMIENTO (usa subset='training')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     train_generator = train_val_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINITIAL_TARGET_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ECG_DATA/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf /content/BeatAI/\n"
      ],
      "metadata": {
        "id": "57JH-0VAhtuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "dataset_path = \"./data/ECG_DATA/\"\n",
        "train_generator, test_generator, val_generator = preprocess_dataset(dataset_path)\n",
        "\n",
        "\n",
        "model = load_model(\"models/ecg_modelv3.h5\")\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Seguir entrenando desde donde quedo\n",
        "epochs = 5\n",
        "history = model.fit(train_generator, epochs= epochs, validation_data=val_generator)\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ULhwr3HNGEnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_graphs(history):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "show_graphs(history)"
      ],
      "metadata": {
        "id": "KpyYu-_37_mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "!git pull\n",
        "\n",
        "# Guardar dentro de la carpeta correcta\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "model.save(\"models/ecg_classification_cnn_model.h5\")\n",
        "github_user = userdata.get(\"github_user\")\n",
        "github_token = userdata.get(\"GITHUB_TOKEN\")\n",
        "github_mail = userdata.get(\"github_mail\")\n",
        "!git config --global user.name \"{github_user}\"\n",
        "!git config --global user.email \"{github_mail}\"\n",
        "repo_url = f\"https://{github_user}:{github_token}@github.com/SantiagoBuffa/BeatAI.git\"\n",
        "!git remote set-url origin $repo_url\n",
        "\n",
        "time_zone = pytz.timezone(\"America/Argentina/Buenos_Aires\")\n",
        "right_now = datetime.now(time_zone)\n",
        "date_and_time = right_now.strftime(\"%d-%m %H:%M\")\n",
        "\n",
        "# Registrar en git y subir\n",
        "!git add models/ecg_classification_cnn_model.h5\n",
        "!git commit -m \"{date_and_time} Continuamos entrenamiento, {epochs} épocas\"\n",
        "!git push origin main\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oEip-QLMHPr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}