{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateollorente/Producto/blob/master/superVectorEvaluacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "import os, json\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n"
      ],
      "metadata": {
        "id": "hpxvGDPxzgUR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SantiagoBuffa/BeatAI.git\n",
        "%cd BeatAI"
      ],
      "metadata": {
        "id": "EGvxjaKHzUxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e549f2b1-5790-47ad-8cdd-a1f5bcf1cbb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BeatAI'...\n",
            "remote: Enumerating objects: 852, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 852 (delta 36), reused 43 (delta 13), pack-reused 762 (from 3)\u001b[K\n",
            "Receiving objects: 100% (852/852), 98.16 MiB | 17.22 MiB/s, done.\n",
            "Resolving deltas: 100% (566/566), done.\n",
            "/content/BeatAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "kaggle_username = userdata.get(\"kaggle_username\")\n",
        "kaggle_key = userdata.get(\"kaggle_key\")\n",
        "\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n",
        "    json.dump({\"username\": kaggle_username, \"key\": kaggle_key}, f)\n",
        "\n",
        "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
        "\n",
        "!pip install kaggle --quiet\n",
        "!kaggle datasets download -d evilspirit05/ecg-analysis -p ./data --unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4XsCfJf5gEg",
        "outputId": "30164b12-9a92-4d8f-9285-ab9047d7a52a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/evilspirit05/ecg-analysis\n",
            "License(s): MIT\n",
            "Downloading ecg-analysis.zip to ./data\n",
            "100% 825M/826M [00:07<00:00, 42.5MB/s]\n",
            "100% 826M/826M [00:07<00:00, 116MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_degradation_to_gray(gray_img):\n",
        "    \"\"\"\n",
        "    Aplica un desenfoque Gaussiano y ruido normal a una imagen\n",
        "    en escala de grises (array de NumPy).\n",
        "    \"\"\"\n",
        "    # 1. Aplicar desenfoque (blur)\n",
        "    blurred = cv2.GaussianBlur(gray_img, (5, 5), 0)\n",
        "\n",
        "    # 2. Añadir ruido\n",
        "    # Convertimos a float32 para sumar el ruido sin problemas de clipping\n",
        "    degraded_float = blurred.astype(np.float32)\n",
        "    noise = np.random.normal(0, 10, gray_img.shape) # Ruido con std dev 10\n",
        "    degraded_float += noise\n",
        "\n",
        "    # 3. Volver al rango 0-255 y al tipo uint8\n",
        "    degraded_clipped = np.clip(degraded_float, 0, 255)\n",
        "\n",
        "    return degraded_clipped.astype(np.uint8)"
      ],
      "metadata": {
        "id": "Q6pOzZehm2Ku"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ecg_to_vector(path, num_rows=4, smooth=True, apply_degradation=False):\n",
        "    \"\"\"\n",
        "    Tu función de extracción de señal, ahora con degradación opcional.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"No se pudo cargar la imagen: {path}\")\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    target_size = (1300, 930)\n",
        "    gray = cv2.resize(gray, target_size, interpolation=cv2.INTER_AREA)\n",
        "    # --- INICIO DE LA MODIFICACIÓN ---\n",
        "    # Si se activa, aplicamos la degradación a la imagen en grises\n",
        "    # ANTES de cualquier otro procesamiento.\n",
        "    if apply_degradation:\n",
        "        gray = apply_degradation_to_gray(gray)\n",
        "    # --- FIN DE LA MODIFICACIÓN ---\n",
        "\n",
        "    # El resto de tu lógica de binarización y limpieza funciona\n",
        "    # ahora sobre la imagen 'gray' (limpia o degradada).\n",
        "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
        "                                   cv2.THRESH_BINARY_INV, 35, 10)\n",
        "\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 1))\n",
        "    clean = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    contours, _ = cv2.findContours(clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Manejar imágenes vacías (sin contornos)\n",
        "    if len(contours) == 0:\n",
        "        # Si no hay contornos, devuelve un vector de ceros\n",
        "        # (o podrías decidir omitir este archivo en la función de carga)\n",
        "        print(f\"Advertencia: No se encontraron contornos en {path}. Saltando.\")\n",
        "        return None # Devolver None para que la función de carga lo omita\n",
        "\n",
        "    x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n",
        "    clean = clean[y:y+h, x:x+w]\n",
        "\n",
        "    height = clean.shape[0]\n",
        "\n",
        "    # Evitar división por cero si la imagen es muy pequeña\n",
        "    if height < num_rows:\n",
        "        print(f\"Advertencia: Altura de contorno ({height}) menor que num_rows en {path}. Saltando.\")\n",
        "        return None\n",
        "\n",
        "    row_height = height // num_rows\n",
        "    signals = []\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        row = clean[i*row_height:(i+1)*row_height, :]\n",
        "        ys = []\n",
        "        for col in range(row.shape[1]):\n",
        "            pixels = np.where(row[:, col] > 0)[0]\n",
        "            if len(pixels) > 0:\n",
        "                ys.append(np.mean(pixels))\n",
        "            else:\n",
        "                ys.append(np.nan)\n",
        "\n",
        "        ys = np.array(ys)\n",
        "        nans = np.isnan(ys)\n",
        "\n",
        "        # Manejar caso donde todos son NaNs\n",
        "        if np.all(nans) or not np.any(~nans):\n",
        "            ys[:] = 0 # Rellenar con ceros si la tira está vacía\n",
        "        elif np.any(nans):\n",
        "            # Interpolar NaNs\n",
        "            ys[nans] = np.interp(np.flatnonzero(nans), np.flatnonzero(~nans), ys[~nans])\n",
        "\n",
        "        # Normalizar la señal de la tira (evitar división por cero)\n",
        "        min_y, max_y = np.min(ys), np.max(ys)\n",
        "        if max_y - min_y > 1e-6: # Un umbral pequeño\n",
        "            ys = (ys - min_y) / (max_y - min_y)\n",
        "        else:\n",
        "            ys[:] = 0.5 # Si la línea es plana, ponerla en el medio\n",
        "\n",
        "        signals.append(ys)\n",
        "\n",
        "    vector = np.concatenate(signals)\n",
        "\n",
        "    if smooth:\n",
        "        vector = cv2.GaussianBlur(vector.reshape(-1, 1), (9, 1), 0).flatten()\n",
        "\n",
        "    return vector"
      ],
      "metadata": {
        "id": "MSRPfRxRx-5H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_vector(v, target_len=4096):\n",
        "    x_old = np.linspace(0, 1, len(v))\n",
        "    x_new = np.linspace(0, 1, target_len)\n",
        "    return np.interp(x_new, x_old, v)"
      ],
      "metadata": {
        "id": "o7XPX53TyKOp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset_1d(dataset_path, target_len=4096, num_rows=4):\n",
        "    \"\"\"\n",
        "    Carga todas las imágenes del dataset y genera vectores 1D concatenados.\n",
        "    - Aplica degradación al conjunto de 'train'.\n",
        "    - NO aplica degradación al conjunto de 'test'.\n",
        "    \"\"\"\n",
        "    train_dir = os.path.join(dataset_path, 'train')\n",
        "    test_dir = os.path.join(dataset_path, 'test')\n",
        "\n",
        "    X_train, y_train = [], []\n",
        "    X_test, y_test = [], []\n",
        "\n",
        "    class_names = sorted(os.listdir(train_dir))\n",
        "    print(f\"📂 Clases detectadas: {class_names}\")\n",
        "\n",
        "    # --- TRAIN (CON DEGRADACIÓN) ---\n",
        "    for label, cls in enumerate(class_names):\n",
        "        cls_dir = os.path.join(train_dir, cls)\n",
        "        for fname in tqdm(os.listdir(cls_dir), desc=f\"Procesando {cls} (train)\"):\n",
        "            path = os.path.join(cls_dir, fname)\n",
        "            if not fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            # Llamamos con apply_degradation=True\n",
        "            vec = ecg_to_vector(path, num_rows=num_rows, apply_degradation=True)\n",
        "\n",
        "            # Omitir si la función devolvió None (por error o sin contornos)\n",
        "            if vec is None:\n",
        "                continue\n",
        "\n",
        "            vec = resize_vector(vec, target_len)\n",
        "            X_train.append(vec)\n",
        "            y_train.append(label)\n",
        "\n",
        "    # --- TEST (SIN DEGRADACIÓN) ---\n",
        "    for label, cls in enumerate(class_names):\n",
        "        cls_dir = os.path.join(test_dir, cls)\n",
        "        for fname in tqdm(os.listdir(cls_dir), desc=f\"Procesando {cls} (test)\"):\n",
        "            path = os.path.join(cls_dir, fname)\n",
        "            if not fname.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            # Llamamos con apply_degradation=False\n",
        "            vec = ecg_to_vector(path, num_rows=num_rows, apply_degradation=False)\n",
        "\n",
        "            if vec is None:\n",
        "                continue\n",
        "\n",
        "            vec = resize_vector(vec, target_len)\n",
        "            X_test.append(vec)\n",
        "            y_test.append(label)\n",
        "\n",
        "    X_train = np.array(X_train)[..., np.newaxis]\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(class_names))\n",
        "    X_test = np.array(X_test)[..., np.newaxis]\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, num_classes=len(class_names))\n",
        "\n",
        "    # --- SPLIT VALIDATION ---\n",
        "    # X_val se creará a partir de X_train, por lo que también\n",
        "    # contendrá imágenes degradadas, lo cual es correcto.\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train.argmax(axis=1)\n",
        "    )\n",
        "\n",
        "    print(f\"✅ X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}\")\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "metadata": {
        "id": "GiNK8_Tfi9Mg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ecg_1d_model(input_length=4096, num_classes=4):\n",
        "    \"\"\"\n",
        "    Construye el modelo CNN 1D.\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        # El Input debe tener la forma (longitud, canales)\n",
        "        layers.Input(shape=(input_length,)),\n",
        "        # Reshape para añadir el canal (necesario para Conv1D)\n",
        "        layers.Reshape((input_length, 1)),\n",
        "\n",
        "        layers.Conv1D(32, 7, activation='relu', padding='same'),\n",
        "        layers.MaxPooling1D(2),\n",
        "\n",
        "        layers.Conv1D(64, 5, activation='relu', padding='same'),\n",
        "        layers.MaxPooling1D(2),\n",
        "\n",
        "        layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "WXns7MhxGzvm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/BeatAI/data/ECG_DATA\"\n",
        "TARGET_VECTOR_LENGTH = 4096\n",
        "NUM_CLASSES = 4 # ¡Asegúrate de que esto coincida con tus datos!\n",
        "\n",
        "# Llamamos a la nueva función de preprocesamiento\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = preprocess_dataset_1d(\n",
        "    dataset_path\n",
        ")\n",
        "\n",
        "# Determinar el número de clases automáticamente\n",
        "if y_train.shape[1] != NUM_CLASSES:\n",
        "    print(f\"Advertencia: NUM_CLASSES era {NUM_CLASSES}, pero los datos tienen {y_train.shape[1]} clases.\")\n",
        "    NUM_CLASSES = y_train.shape[1]\n",
        "\n",
        "# Construir el modelo\n",
        "model = build_ecg_1d_model(\n",
        "    input_length=TARGET_VECTOR_LENGTH,\n",
        "    num_classes=NUM_CLASSES\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Entrenar el modelo\n",
        "if X_train.shape[0] > 0:\n",
        "    print(\"\\nIniciando entrenamiento...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100, # Aumentado a 20 épocas\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
        "    )\n",
        "\n",
        "    # Evaluar en el conjunto de test (limpio)\n",
        "    print(\"\\nEvaluando en el conjunto de test (limpio)...\")\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "else:\n",
        "    print(\"No se cargaron datos de entrenamiento. Revisa la ruta y los archivos.\")"
      ],
      "metadata": {
        "id": "8gSRW01TBn-h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ac632c1-bd7d-484c-f709-0b37426ec75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Clases detectadas: ['ECG Images of Myocardial Infarction Patients (240x12=2880)', 'ECG Images of Patient that have History of MI (172x12=2064)', 'ECG Images of Patient that have abnormal heartbeat (233x12=2796)', 'Normal Person ECG Images (284x12=3408)']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Procesando ECG Images of Myocardial Infarction Patients (240x12=2880) (train): 100%|██████████| 956/956 [03:20<00:00,  4.76it/s]\n",
            "Procesando ECG Images of Patient that have History of MI (172x12=2064) (train): 100%|██████████| 516/516 [01:43<00:00,  4.96it/s]\n",
            "Procesando ECG Images of Patient that have abnormal heartbeat (233x12=2796) (train): 100%|██████████| 699/699 [02:19<00:00,  5.01it/s]\n",
            "Procesando Normal Person ECG Images (284x12=3408) (train): 100%|██████████| 852/852 [02:50<00:00,  4.99it/s]\n",
            "Procesando ECG Images of Myocardial Infarction Patients (240x12=2880) (test): 100%|██████████| 239/239 [00:31<00:00,  7.57it/s]\n",
            "Procesando ECG Images of Patient that have History of MI (172x12=2064) (test): 100%|██████████| 172/172 [00:21<00:00,  8.03it/s]\n",
            "Procesando ECG Images of Patient that have abnormal heartbeat (233x12=2796) (test): 100%|██████████| 233/233 [00:32<00:00,  7.09it/s]\n",
            "Procesando Normal Person ECG Images (284x12=3408) (test): 100%|██████████| 284/284 [00:36<00:00,  7.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ X_train: (2418, 4096, 1), X_val: (605, 4096, 1), X_test: (928, 4096, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m, \u001b[38;5;34m1\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m10,304\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m24,704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m260\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,780\u001b[0m (171.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,780</span> (171.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,780\u001b[0m (171.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,780</span> (171.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando entrenamiento...\n",
            "Epoch 1/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 511ms/step - accuracy: 0.2877 - loss: 1.3732 - val_accuracy: 0.3157 - val_loss: 1.3652\n",
            "Epoch 2/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 415ms/step - accuracy: 0.2846 - loss: 1.3655 - val_accuracy: 0.3157 - val_loss: 1.3580\n",
            "Epoch 3/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 403ms/step - accuracy: 0.3086 - loss: 1.3616 - val_accuracy: 0.3157 - val_loss: 1.3424\n",
            "Epoch 4/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 397ms/step - accuracy: 0.3279 - loss: 1.3356 - val_accuracy: 0.3240 - val_loss: 1.3380\n",
            "Epoch 5/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 387ms/step - accuracy: 0.3395 - loss: 1.3418 - val_accuracy: 0.3174 - val_loss: 1.3349\n",
            "Epoch 6/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 387ms/step - accuracy: 0.3323 - loss: 1.3343 - val_accuracy: 0.3107 - val_loss: 1.3402\n",
            "Epoch 7/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 398ms/step - accuracy: 0.3318 - loss: 1.3313 - val_accuracy: 0.3653 - val_loss: 1.3241\n",
            "Epoch 8/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 410ms/step - accuracy: 0.3657 - loss: 1.3218 - val_accuracy: 0.3669 - val_loss: 1.3169\n",
            "Epoch 9/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 389ms/step - accuracy: 0.3545 - loss: 1.3201 - val_accuracy: 0.3653 - val_loss: 1.2930\n",
            "Epoch 10/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 391ms/step - accuracy: 0.3721 - loss: 1.3003 - val_accuracy: 0.3223 - val_loss: 1.3277\n",
            "Epoch 11/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 416ms/step - accuracy: 0.3400 - loss: 1.3177 - val_accuracy: 0.3736 - val_loss: 1.2799\n",
            "Epoch 12/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 391ms/step - accuracy: 0.3771 - loss: 1.2889 - val_accuracy: 0.3521 - val_loss: 1.2765\n",
            "Epoch 13/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 403ms/step - accuracy: 0.3909 - loss: 1.2649 - val_accuracy: 0.3686 - val_loss: 1.2577\n",
            "Epoch 14/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 389ms/step - accuracy: 0.3689 - loss: 1.2771 - val_accuracy: 0.3802 - val_loss: 1.2747\n",
            "Epoch 15/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 399ms/step - accuracy: 0.3834 - loss: 1.2719 - val_accuracy: 0.3818 - val_loss: 1.2521\n",
            "Epoch 16/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 389ms/step - accuracy: 0.3741 - loss: 1.2627 - val_accuracy: 0.4050 - val_loss: 1.2879\n",
            "Epoch 17/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 484ms/step - accuracy: 0.4020 - loss: 1.2569 - val_accuracy: 0.3868 - val_loss: 1.2597\n",
            "Epoch 18/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 453ms/step - accuracy: 0.4109 - loss: 1.2661 - val_accuracy: 0.4298 - val_loss: 1.2593\n",
            "Epoch 19/20\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 570ms/step - accuracy: 0.4070 - loss: 1.2487 - val_accuracy: 0.4380 - val_loss: 1.2475\n",
            "Epoch 20/20\n",
            "\u001b[1m50/76\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 403ms/step - accuracy: 0.4220 - loss: 1.2390"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_graphs(history):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "show_graphs(history)"
      ],
      "metadata": {
        "id": "KpyYu-_37_mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"models/ecg_modelVectore.h5\")"
      ],
      "metadata": {
        "id": "s7RINUD71lZm",
        "outputId": "7723c3f3-8999-40ca-e1fd-8ba54a86a8bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git push origin main"
      ],
      "metadata": {
        "id": "y54Mv27aEtR6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}