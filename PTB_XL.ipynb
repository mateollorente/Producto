{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZ5G1AwrmWjSxnFU17hU2+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mateollorente/Producto/blob/master/PTB_XL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-uzXXD8vomzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1b3640-4f7e-43cb-e590-704ad8842a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-27 02:16:06--  https://physionet.org/static/published-projects/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip\n",
            "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
            "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://physionet.org/content/ptb-xl/get-zip/1.0.3/ [following]\n",
            "--2025-10-27 02:16:07--  https://physionet.org/content/ptb-xl/get-zip/1.0.3/\n",
            "Reusing existing connection to physionet.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1839504686 (1.7G) [application/zip]\n",
            "Saving to: ‘ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip’\n",
            "\n",
            "                  p   2%[                    ]  39.19M   413KB/s    eta 74m 47s^C\n",
            "[ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip or\n",
            "        ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip.zip, and cannot find ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip.ZIP, period.\n",
            "ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip\n",
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "# 1. Descargar el archivo ZIP directamente desde PhysioNet\n",
        "# Este comando descarga la versión 1.0.3 del dataset.\n",
        "!wget -N https://physionet.org/static/published-projects/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip\n",
        "\n",
        "# 2. Descomprimir el archivo ZIP\n",
        "# El comando 'unzip -q' descomprime silenciosamente (sin listar todos los archivos).\n",
        "# Los archivos se extraerán en la carpeta actual donde ejecutes el notebook.\n",
        "!unzip -q ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip\n",
        "\n",
        "# 3. (Opcional) Verificar que los archivos existen\n",
        "# Listar el contenido para confirmar que se descomprimió correctamente.\n",
        "# Deberías ver las carpetas 'records100', 'records500' y los archivos .csv\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wfdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import ast # Para procesar el diccionario de scp_codes\n",
        "\n",
        "# --- 1. Definir Constantes y Cargar Metadatos ---\n",
        "SAMPLING_RATE = 100\n",
        "DATA_PATH = './' # Ruta base donde están los archivos descomprimidos\n",
        "RECORDS_PATH = f'{DATA_PATH}records{SAMPLING_RATE}/' # Ruta a los archivos WFDB\n",
        "\n",
        "# Cargar metadatos\n",
        "metadata_df = pd.read_csv(f'{DATA_PATH}ptbxl_database.csv', index_col='ecg_id')\n",
        "scp_df = pd.read_csv(f'{DATA_PATH}scp_statements.csv', index_col=0)\n",
        "\n",
        "# Filtrar solo códigos SCP de diagnóstico para las superclases\n",
        "scp_df = scp_df[scp_df.diagnostic == 1]\n",
        "\n",
        "# --- 2. Función para Mapear Códigos SCP a Superclases ---\n",
        "def aggregate_diagnostic(scp_codes_dict):\n",
        "    \"\"\"\n",
        "    Mapea los códigos SCP de un ECG a su superclase diagnóstica principal.\n",
        "    Prioriza NORM si está presente, de lo contrario, toma la primera superclase encontrada.\n",
        "    Devuelve NaN si no hay códigos de diagnóstico mapeables.\n",
        "    \"\"\"\n",
        "    # Mapeo de superclases (ajustar si es necesario)\n",
        "    superclass_map = {\n",
        "        'NORM': 'NORM', # Normal ECG\n",
        "        'MI': 'MI',     # Myocardial Infarction\n",
        "        'STTC': 'STTC', # ST/T Change\n",
        "        'CD': 'CD',     # Conduction Disturbance\n",
        "        'HYP': 'HYP'    # Hypertrophy\n",
        "    }\n",
        "\n",
        "    # Añadir columna de superclase al DataFrame scp_df si no existe\n",
        "    # (Esto puede requerir un mapeo manual o usar las columnas existentes)\n",
        "    # Por simplicidad, asumimos que scp_df tiene una columna 'diagnostic_class'\n",
        "    # que contiene NORM, MI, STTC, CD, HYP. Si no, necesitas crearla.\n",
        "    # Ejemplo: scp_df['diagnostic_class'] = scp_df['diagnostic_subclass'].map(subclass_to_superclass_dict)\n",
        "\n",
        "    if not isinstance(scp_codes_dict, dict):\n",
        "      return np.nan # No es un diccionario válido\n",
        "\n",
        "    # Buscar si 'NORM' está presente con alta probabilidad\n",
        "    if 'NORM' in scp_codes_dict and scp_codes_dict['NORM'] >= 50:\n",
        "         return 'NORM'\n",
        "\n",
        "    # Si no es NORM, buscar la primera superclase diagnóstica presente\n",
        "    for code, likelihood in scp_codes_dict.items():\n",
        "        if code in scp_df.index and likelihood >= 50: # Considerar solo si la probabilidad es alta\n",
        "            # Asume que scp_df tiene la columna 'diagnostic_class'\n",
        "            # Necesitas asegurarte de que esta columna exista y esté poblada\n",
        "            # con NORM, MI, STTC, CD, HYP\n",
        "            if 'diagnostic_class' in scp_df.columns:\n",
        "                super_class = scp_df.loc[code, 'diagnostic_class']\n",
        "                if super_class in superclass_map:\n",
        "                    return super_class\n",
        "            else:\n",
        "                 # Si no existe 'diagnostic_class', mapear manualmente aquí o retornar NaN\n",
        "                 # print(f\"Advertencia: Columna 'diagnostic_class' no encontrada en scp_df.\")\n",
        "                 # return np.nan # O intentar un mapeo manual si conoces los códigos\n",
        "                 pass # Evitar que falle si la columna no existe\n",
        "\n",
        "    return np.nan # Si no se encuentra ninguna superclase válida\n",
        "\n",
        "# --- 3. Aplicar Mapeo de Etiquetas y Dividir Datos ---\n",
        "\n",
        "# Convertir la columna 'scp_codes' de string a diccionario\n",
        "metadata_df['scp_codes'] = metadata_df.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "# Aplicar la función para obtener la etiqueta de superclase\n",
        "metadata_df['diagnostic_superclass'] = metadata_df.scp_codes.apply(aggregate_diagnostic)\n",
        "\n",
        "# Eliminar filas donde no se pudo asignar una superclase\n",
        "metadata_df = metadata_df.dropna(subset=['diagnostic_superclass'])\n",
        "\n",
        "# Crear un mapeo numérico para las clases\n",
        "unique_classes = metadata_df.diagnostic_superclass.unique()\n",
        "class_map = {label: i for i, label in enumerate(unique_classes)}\n",
        "metadata_df['label_id'] = metadata_df.diagnostic_superclass.map(class_map)\n",
        "num_classes = len(unique_classes)\n",
        "\n",
        "print(f\"Mapeo de Clases: {class_map}\")\n",
        "\n",
        "# Dividir según los folds recomendados\n",
        "train_df = metadata_df[metadata_df.strat_fold <= 8]\n",
        "val_df = metadata_df[metadata_df.strat_fold == 9]\n",
        "test_df = metadata_df[metadata_df.strat_fold == 10]\n",
        "\n",
        "print(f\"Tamaños -> Entrenamiento: {len(train_df)}, Validación: {len(val_df)}, Prueba: {len(test_df)}\")\n",
        "\n",
        "# --- 4. Función para Cargar y Normalizar Señales ---\n",
        "\n",
        "def load_and_preprocess_data(df, records_path, sampling_rate):\n",
        "    \"\"\"Carga señales, normaliza y convierte etiquetas.\"\"\"\n",
        "\n",
        "    # Cargar señales (elige filename_lr para 100Hz)\n",
        "    signals = [wfdb.rdsamp(records_path + f)[0] for f in df.filename_lr]\n",
        "    X = np.array(signals)\n",
        "\n",
        "    # Normalizar cada señal (lead por lead)\n",
        "    # (Muestras, Longitud, Derivaciones) -> (Muestras, Longitud_Normalizada, Derivaciones)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = np.zeros_like(X, dtype=np.float32) # Usar float32 para Keras\n",
        "    for i in range(X.shape[0]):\n",
        "         # Normaliza cada derivación (lead) independientemente\n",
        "         X_scaled[i] = scaler.fit_transform(X[i])\n",
        "\n",
        "    # Obtener etiquetas numéricas y convertir a one-hot\n",
        "    y = df.label_id.values\n",
        "    y_cat = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "    return X_scaled, y_cat\n",
        "\n",
        "# --- 5. Ejecutar Carga y Preprocesamiento ---\n",
        "print(\"Cargando y preprocesando datos de entrenamiento...\")\n",
        "X_train, y_train = load_and_preprocess_data(train_df, RECORDS_PATH, SAMPLING_RATE)\n",
        "print(\"Cargando y preprocesando datos de validación...\")\n",
        "X_val, y_val = load_and_preprocess_data(val_df, RECORDS_PATH, SAMPLING_RATE)\n",
        "print(\"Cargando y preprocesando datos de prueba...\")\n",
        "X_test, y_test = load_and_preprocess_data(test_df, RECORDS_PATH, SAMPLING_RATE)\n",
        "\n",
        "print(\"\\n--- Formas Finales ---\")\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "m9B31VjVbA4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization\n",
        "\n",
        "def build_conv1d_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=10, activation='relu', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=3),\n",
        "\n",
        "        Conv1D(filters=128, kernel_size=10, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=3),\n",
        "\n",
        "        Conv1D(filters=128, kernel_size=10, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        GlobalAveragePooling1D(), # Reduce la dimensionalidad antes de Dense\n",
        "\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax') # Softmax para multiclase\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# --- Uso ---\n",
        "# input_shape = (X_train.shape[1], X_train.shape[2]) # (longitud_secuencia, num_derivaciones)\n",
        "# num_classes = y_train.shape[1]\n",
        "# model = build_conv1d_model(input_shape, num_classes)\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "7OxlqPLZbICR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}